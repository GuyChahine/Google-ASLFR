{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Settings(input_folder='./data/', model_folder='./models/', batch_size=2, sos_token=59, eos_token=60, pad_token=61, max_frames=250, nb_feature=164, max_phrases=32, nb_token=61, x_shape=(128, 164))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Settings():\n",
    "    \n",
    "    input_folder: str = \"./data/\"\n",
    "    model_folder: str = \"./models/\"\n",
    "    \n",
    "    batch_size: int = 2\n",
    "    \n",
    "    sos_token: int = 59\n",
    "    eos_token: int = 60\n",
    "    pad_token: int = 61\n",
    "    \n",
    "    max_frames: int = 250\n",
    "    nb_feature: int = 164\n",
    "    max_phrases: int = 31 + 1\n",
    "    nb_token: int = 58 + 3\n",
    "    \n",
    "    x_shape: tuple[int, int] = (128,nb_feature)\n",
    "    \n",
    "    encoder_feature = 256\n",
    "    encoder_block = 2\n",
    "    encoder_head = 8\n",
    "    \n",
    "    decoder_feature = 256\n",
    "    decoder_block = 2\n",
    "    decoder_head = 8\n",
    "    \n",
    "S = Settings()\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DS, self).__init__()\n",
    "        \n",
    "        self.xs = torch.load(S.input_folder + \"x.torch\")\n",
    "        self.ys = torch.load(S.input_folder + \"y.torch\")\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.xs[index], self.ys[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_3d_tensor_shape(T: torch.Tensor, dim: int):\n",
    "    T = T.clone()\n",
    "    if dim == 0: T = T[:, 0, 0]\n",
    "    elif dim == 1: T = T[0, :, 0]\n",
    "    elif dim == 2: T = T[0, 0, :]\n",
    "    T[:] = 1.\n",
    "    T = T.sum()\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessing(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DataProcessing, self).__init__()\n",
    "        \n",
    "        self.max_frames = torch.tensor(S.max_frames)\n",
    "        self.x_shape = S.x_shape\n",
    "        self.zero_tensor = torch.tensor(0.)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) <= 2: x = x[None]\n",
    "        \n",
    "        x = torch.where(x.isnan(), self.zero_tensor, x)\n",
    "        \n",
    "        nb_frame = find_3d_tensor_shape(x, 1).to(torch.int64)\n",
    "        pad_value = torch.where(nb_frame < self.max_frames, self.max_frames - nb_frame, self.zero_tensor.to(torch.int64))\n",
    "        x = F.pad(x, (0,0,0,pad_value), \"constant\", self.zero_tensor)\n",
    "        x = F.interpolate(x[None], self.x_shape)[0]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(T, nb_class, batch_first=True):\n",
    "    assert len(T.unique()) <= nb_class, \"nb_class should be higher then number of unique element in tensor T\"\n",
    "    T_dtype = T.dtype\n",
    "    if not batch_first: T = T[None]\n",
    "    out = []\n",
    "    for batch in T:\n",
    "        out.append(torch.stack([torch.where(batch == uniq, 1, 0) for uniq in range(nb_class)]).T)\n",
    "    out = torch.stack(out)\n",
    "    if not batch_first: out = out[0]\n",
    "    return out.to(T_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FrameEmbedding, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(S.x_shape[1], S.encoder_feature, False)\n",
    "        self.l2 = nn.Linear(S.encoder_feature, S.encoder_feature, False)\n",
    "        \n",
    "        self.pe1 = nn.Parameter(torch.zeros((S.x_shape[0], S.encoder_feature)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        \n",
    "        x = x + self.pe1\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.device = \"cpu\"\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        depth = embed_dim // num_heads\n",
    "        \n",
    "        self.lq = nn.ModuleList([nn.Linear(embed_dim, depth) for _ in range(num_heads)])\n",
    "        self.lk = nn.ModuleList([nn.Linear(embed_dim, depth) for _ in range(num_heads)])\n",
    "        self.lv = nn.ModuleList([nn.Linear(embed_dim, depth) for _ in range(num_heads)])\n",
    "        \n",
    "        self.lo = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, q, k, v, attn_mask):\n",
    "        qkt = torch.bmm(q, k.permute(0,2,1))\n",
    "        scaling_factor = find_3d_tensor_shape(q, 2).float().to(q.device)\n",
    "        scaled_qkt = qkt / scaling_factor\n",
    "        \n",
    "        if attn_mask != None: attn_mask = torch.where(attn_mask == 0, torch.FloatTensor([-torch.inf]), torch.FloatTensor([0]))\n",
    "        else: attn_mask = torch.zeros(*scaled_qkt.shape).to(q.device)\n",
    "        attn_qkt = scaled_qkt + attn_mask\n",
    "        softmax_qkt = F.softmax(attn_qkt, 1)\n",
    "        \n",
    "        output = torch.bmm(softmax_qkt, v)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        \n",
    "        multi_attn = [self.scaled_dot_product_attention(lq(q), lk(k), lv(v), attn_mask)\n",
    "            for lq, lk, lv in zip(self.lq, self.lk, self.lv)]\n",
    "        \n",
    "        multi_head = torch.cat(multi_attn, 2)\n",
    "        multi_head_attn = self.lo(multi_head)\n",
    "        return multi_head_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList([self.encorder_block() for _ in range(S.encoder_block)])\n",
    "        \n",
    "    def encorder_block(self):\n",
    "        return nn.ModuleList([\n",
    "            MultiHeadAttention(S.encoder_feature, S.encoder_head),\n",
    "            nn.LayerNorm(S.encoder_feature),\n",
    "            nn.Linear(S.encoder_feature, S.encoder_feature),\n",
    "            nn.Linear(S.encoder_feature, S.encoder_feature),\n",
    "            nn.LayerNorm(S.encoder_feature),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        attn_mask = x.sum(2)\n",
    "        attn_mask = torch.where(attn_mask == 0, torch.FloatTensor([0.]), torch.FloatTensor([1.]))\n",
    "        attn_mask = attn_mask[:,:,None].repeat(1,1,S.x_shape[0])\n",
    "        \n",
    "        for mha, ln1, l1, l2, ln2 in self.blocks:\n",
    "            \n",
    "            _x = x\n",
    "            x = mha(x, x, x, attn_mask)\n",
    "            x = ln1(x + _x)\n",
    "            \n",
    "            _x = x\n",
    "            x = F.gelu(l1(x))\n",
    "            x = l2(x)\n",
    "            x = ln2(x + _x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhraseEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PhraseEmbedding, self).__init__()\n",
    "        \n",
    "        self.emb1 = nn.Embedding(S.nb_token+1, S.decoder_feature)\n",
    "        self.pe1 = nn.Parameter(torch.zeros((S.x_shape[0], S.decoder_feature)))\n",
    "        \n",
    "    def forward(self, y):\n",
    "        y = F.pad(y, (1,0,0,0), \"constant\", S.sos_token)\n",
    "        y = F.pad(y, (0, S.x_shape[0] - y.shape[1],0,0), \"constant\", S.pad_token)\n",
    "        y = self.emb1(y.to(torch.int64))\n",
    "        \n",
    "        y = y + self.pe1\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.causal_mha = MultiHeadAttention(S.decoder_feature, S.decoder_head)\n",
    "        self.causal_ln = nn.LayerNorm(S.decoder_feature)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([self.decoder_block() for _ in range(S.decoder_block)])\n",
    "        \n",
    "    def decoder_block(self):\n",
    "        return nn.ModuleList([\n",
    "            MultiHeadAttention(S.decoder_feature, S.decoder_head),\n",
    "            nn.LayerNorm(S.decoder_feature),\n",
    "            nn.Linear(S.decoder_feature, S.decoder_feature),\n",
    "            nn.Linear(S.decoder_feature, S.decoder_feature),\n",
    "            nn.LayerNorm(S.decoder_feature),\n",
    "        ])\n",
    "        \n",
    "    def forward(self, encoder_output, x):\n",
    "        \n",
    "        causal_mask = torch.arange(S.x_shape[0])[:, None] >= torch.arange(S.x_shape[0])\n",
    "        causal_mask = causal_mask.float().repeat(x.shape[0],1,1).to(x.device)\n",
    "        \n",
    "        _x = x\n",
    "        self.causal_mha(x, x, x, causal_mask)\n",
    "        x = self.causal_ln(x + _x)\n",
    "        \n",
    "        for mha, ln1, l1, l2, ln2 in self.blocks:\n",
    "            \n",
    "            _x = x\n",
    "            x = mha(x, encoder_output, encoder_output, causal_mask)\n",
    "            x = ln1(x + _x)\n",
    "            \n",
    "            _x = x\n",
    "            x = F.gelu(l1(x))\n",
    "            x = l2(x)\n",
    "            x = ln2(x + _x)\n",
    "        \n",
    "        x = x[:,:S.max_phrases,:]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(S.decoder_feature, S.nb_token)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.softmax(x, 1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "cls = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.DP = DataProcessing()\n",
    "        self.FE = FrameEmbedding()\n",
    "        self.ENC = Encoder()\n",
    "        self.PE = PhraseEmbedding()\n",
    "        self.DEC = Decoder()\n",
    "        self.CLS = Classifier()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x = self.DP(x)\n",
    "        x = self.FE(x)\n",
    "        x = self.ENC(x)\n",
    "        \n",
    "        y = self.PE(y)\n",
    "        x = self.DEC(x, y)\n",
    "        x = self.CLS(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3,250,164)\n",
    "y = torch.randint(0,59,(3,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 61])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(x, y)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, train_model):\n",
    "        super(InferenceModel, self).__init__()\n",
    "        \n",
    "        self.DP = train_model.DP\n",
    "        self.FE = train_model.FE\n",
    "        self.ENC = train_model.ENC\n",
    "        self.PE = train_model.PE\n",
    "        self.DEC = train_model.DEC\n",
    "        self.CLS = train_model.CLS\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.DP(x)\n",
    "        x = self.FE(x)\n",
    "        enc_out = self.ENC(x)\n",
    "        \n",
    "        phrase = torch.full((1,S.max_phrases), S.pad_token).to(x.device)\n",
    "        \n",
    "        for i in range(S.max_phrases):\n",
    "            y = self.PE(phrase)\n",
    "            x = self.DEC(enc_out, y)\n",
    "            x = self.CLS(x)\n",
    "            \n",
    "            phrase[:, :i+1] = x.argmax(2)[:, :i+1]\n",
    "            \n",
    "        return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_model = InferenceModel(model).eval()\n",
    "x_inf = torch.randn(12,164)\n",
    "output_inf = inference_model(x_inf)\n",
    "output_inf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13757/219977612.py:17: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = F.pad(x, (0,0,0,pad_value), \"constant\", self.zero_tensor)\n",
      "/home/guy/.local/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:306: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/guy/.local/lib/python3.10/site-packages/torch/onnx/utils.py:689: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guy/.local/lib/python3.10/site-packages/torch/onnx/utils.py:1186: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    {\"x\": x, \"y\": y},\n",
    "    S.model_folder + \"v2.onnx\",\n",
    "    input_names=[\"x\", \"y\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 17:42:13.203354: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-02 17:42:13.668540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/guy/.local/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(S.model_folder + \"v2.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 17:42:15.331815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-02 17:42:15.332144: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-07-02 17:42:26.325275: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'x' with dtype float and shape [3,250,164]\n",
      "\t [[{{node x}}]]\n",
      "2023-07-02 17:42:26.325339: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor '10520' with dtype float and shape [256]\n",
      "\t [[{{node 10520}}]]\n",
      "2023-07-02 17:42:26.325380: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor '10746' with dtype float and shape [62,256]\n",
      "\t [[{{node 10746}}]]\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "2023-07-02 17:42:33.802071: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'serving_default_x' with dtype float and shape [3,250,164]\n",
      "\t [[{{node serving_default_x}}]]\n",
      "2023-07-02 17:42:34.143522: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'serving_default_y' with dtype int64 and shape [3,30]\n",
      "\t [[{{node serving_default_y}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/v2_tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/v2_tf/assets\n"
     ]
    }
   ],
   "source": [
    "prepare(onnx_model).export_graph(S.model_folder + \"v2_tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = tf.saved_model.load(S.model_folder + \"v2_tf/\")\n",
    "tf_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_x = tf.cast(tf.convert_to_tensor(np.random.random((3,250,164))), tf.float32)\n",
    "tf_y = tf.cast(tf.convert_to_tensor(np.random.randint(0, 59, (3,30))), tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 07:46:42.641003: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor '8967' with dtype float and shape [62,256]\n",
      "\t [[{{node 8967}}]]\n",
      "2023-06-30 07:46:42.789127: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: StatefulPartitionedCall/assert_equal_3/Assert/AssertGuard/branch_executed/_257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 32, 61])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf_model(x=tf_x, y=tf_y)\n",
    "output[\"output\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(S.model_folder + \"v2_tf/\")\n",
    "converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 07:46:43.687791: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'serving_default_y' with dtype int64 and shape [3,30]\n",
      "\t [[{{node serving_default_y}}]]\n",
      "2023-06-30 07:46:43.719047: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-06-30 07:46:43.719070: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-06-30 07:46:43.719359: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./models/v2_tf/\n",
      "2023-06-30 07:46:43.724031: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-06-30 07:46:43.724053: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./models/v2_tf/\n",
      "2023-06-30 07:46:43.735005: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
      "2023-06-30 07:46:43.737715: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-06-30 07:46:43.782225: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: ./models/v2_tf/\n",
      "2023-06-30 07:46:43.827159: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 107799 microseconds.\n",
      "2023-06-30 07:46:43.940074: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-30 07:46:44.450371: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2051] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexErf\n",
      "Details:\n",
      "\ttf.Erf(tensor<3x128x256xf32>) -> (tensor<3x128x256xf32>) : {device = \"\"}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n",
      "2023-06-30 07:46:44.450458: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2116] Estimated count of arithmetic ops: 1.607 G  ops, equivalently 0.803 G  MACs\n"
     ]
    }
   ],
   "source": [
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(S.model_folder + \"v2.tflite\", \"wb\") as f: f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
