{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from json import load\n",
    "from os import listdir\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_search(df, keyword):\n",
    "    if type(keyword) == str: return df.loc[:, ([c for c in df.columns if keyword in c])]\n",
    "    elif type(keyword) == list: return df.loc[:, ([c for c in df.columns if any([k in c for k in keyword])])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(T, nb_class, batch_first=True):\n",
    "    assert len(T.unique()) <= nb_class, \"nb_class should be higher then number of unique element in tensor T\"\n",
    "    T_dtype = T.dtype\n",
    "    if not batch_first: T = T.unsqueeze(0)\n",
    "    out = []\n",
    "    for batch in T:\n",
    "        out.append(torch.stack([torch.where(batch == uniq, 1, 0) for uniq in range(nb_class)]).T)\n",
    "    out = torch.stack(out)\n",
    "    if not batch_first: out = out[0]\n",
    "    return out.to(T_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTP():\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CTP, self).__init__()\n",
    "        \n",
    "        with open (\"./data/character_to_prediction_index.json\", \"r\") as f: self.character_map = load(f)\n",
    "        self.rev_character_map = {j:i for i,j in self.character_map.items()}\n",
    "        \n",
    "    def string_to_list_pred(self, string):\n",
    "        return [self.char_to_pred(char) for char in string]\n",
    "        \n",
    "    def list_pred_to_string(self, list_pred):\n",
    "        return \"\".join([self.pred_to_char(p) for p in list_pred])\n",
    "        \n",
    "    def char_to_pred(self, char):\n",
    "        return self.character_map[char]\n",
    "    \n",
    "    def pred_to_char(self, pred):\n",
    "        return self.rev_character_map[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner(CTP):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nans_management = \"zero\",\n",
    "    ):\n",
    "        super(DataCleaner, self).__init__()\n",
    "        \n",
    "        self.nans_management_map = {\n",
    "            \"clear\": self.clear_nans,\n",
    "            \"zero\": self.zero_nans,\n",
    "        }\n",
    "        self.nans_management = nans_management\n",
    "    \n",
    "        with open(\"./data/dataset_infos.json\", \"r\") as f: self.dataset_infos = load(f)\n",
    "        \n",
    "        self.max_char_length = len(self.character_map)\n",
    "        self.space_char_tensor = torch.zeros(self.max_char_length)\n",
    "        self.space_char_tensor[0] = 1\n",
    "    \n",
    "    def __call__(self, df, y):\n",
    "        \n",
    "        dominant_hand, n_dominant_value = self.find_dominant_hand(df)\n",
    "        df = columns_search(df, [dominant_hand])\n",
    "        df = self.nans_management_map[self.nans_management](df)\n",
    "        x = torch.FloatTensor(df.values)\n",
    "        x = self.pad_x_tensor(x)\n",
    "\n",
    "        y = \"S\" + y + \"E\"\n",
    "        y = self.pad_y_string(y)\n",
    "        y = torch.FloatTensor(self.string_to_list_pred(y))\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "    def find_dominant_hand(self, df):\n",
    "        right_hand_value = columns_search(df, \"right_hand\").notna().sum().sum()\n",
    "        left_hand_value = columns_search(df, \"left_hand\").notna().sum().sum()\n",
    "        \n",
    "        return (\"right_hand\", right_hand_value) if right_hand_value >= left_hand_value else (\"left_hand\", left_hand_value)\n",
    "    \n",
    "    def clear_nans(self, df):\n",
    "        return df[df.notna().all(1)]\n",
    "    \n",
    "    def zero_nans(self, df):\n",
    "        return df.fillna(0)\n",
    "    \n",
    "    def pad_x_tensor(self, x):\n",
    "        pad_length = self.dataset_infos[\"max_frame_nans\"] - x.shape[0]\n",
    "        return F.pad(x, (0, 0, 0, pad_length), \"constant\", 0)\n",
    "    \n",
    "    def pad_y_string(self, y):\n",
    "        pad_length = self.dataset_infos[\"max_length_phrase\"] + 2 - len(y)\n",
    "        return y + \"\".join([\"P\" for _ in range(pad_length)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path = \"./data/splited_data/\",\n",
    "    ):\n",
    "        super(DS, self).__init__()\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        \n",
    "        self.DC = DataCleaner()\n",
    "        \n",
    "        self.labels = pd.read_csv(\"./data/train.csv\")[[\"sequence_id\", \"phrase\"]]\n",
    "        self.labels_length = self.labels.shape[0]\n",
    "        \n",
    "        column_data_example = pd.read_parquet(f\"{data_path}{self.labels.loc[0, 'sequence_id']}.parquet\").columns\n",
    "        self.usefull_columns = [c for c in column_data_example if any([k in c for k in [\"hand\"]])]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        print(index, end=\"\\r\")\n",
    "        sequence_id, y = self.labels.loc[index].to_list()\n",
    "        data = pd.read_parquet(f\"{self.data_path}{sequence_id}.parquet\", columns=self.usefull_columns)\n",
    "\n",
    "        return self.DC(data, y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.labels_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSProcessed(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path = \"./data/processed_data/\",\n",
    "    ):\n",
    "        super(DSProcessed, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.files = listdir(data_path)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.files[index]\n",
    "        with open(self.data_path + file_name, \"rb\") as f: return torch.load(f)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        x_shape,\n",
    "        y_shape,\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.x_shape = x_shape\n",
    "        self.y_shape = y_shape\n",
    "\n",
    "        #Embedding\n",
    "        \n",
    "        self.l1 = nn.Linear(self.x_shape[2], 256, False)\n",
    "        self.l2 = nn.Linear(256, 256, False)\n",
    "        \n",
    "        self.pe1 = nn.Parameter(torch.zeros((self.x_shape[1], 256)))\n",
    "        \n",
    "        # Encoder\n",
    "        self.attn1 = nn.MultiheadAttention(256, 8, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(256)\n",
    "        \n",
    "        self.l4 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 256)\n",
    "        self.ln2 = nn.LayerNorm(256)\n",
    "        \n",
    "        self.attn2 = nn.MultiheadAttention(256, 8, batch_first=True)\n",
    "        self.ln3 = nn.LayerNorm(256)\n",
    "        \n",
    "        self.l5 = nn.Linear(256, 256)\n",
    "        self.l6 = nn.Linear(256, 256)\n",
    "        self.ln4 = nn.LayerNorm(256)\n",
    "        \n",
    "        # Phrase Embedding\n",
    "        \n",
    "        self.emb1 = nn.Embedding(62, 256)\n",
    "        \n",
    "        # Decoder\n",
    "        self.pe2 = nn.Parameter(torch.zeros((self.x_shape[1], 256)))\n",
    "        \n",
    "        self.attn3 = nn.MultiheadAttention(256, 8, batch_first=True)\n",
    "        self.ln5 = nn.LayerNorm(256)\n",
    "        \n",
    "        self.attn4 = nn.MultiheadAttention(256, 8, batch_first=True)\n",
    "        self.ln6 = nn.LayerNorm(256)\n",
    "        \n",
    "        self.l7 = nn.Linear(256, 256)\n",
    "        self.l8 = nn.Linear(256, 256)\n",
    "        self.ln7 = nn.LayerNorm(256)\n",
    "        \n",
    "        self.attn5 = nn.MultiheadAttention(256, 8, batch_first=True)\n",
    "        self.ln8 = nn.LayerNorm(256)\n",
    "        \n",
    "        self.l9 = nn.Linear(256, 256)\n",
    "        self.l10 = nn.Linear(256, 256)\n",
    "        self.ln9 = nn.LayerNorm(256)\n",
    "        \n",
    "        # Classifier\n",
    "        self.l11 = nn.Linear(256, self.y_shape[1]+1)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        #Embedding\n",
    "        \n",
    "        attn_mask = x.clone().sum(2)\n",
    "        attn_mask = torch.where(attn_mask == 0, 0., 1.)\n",
    "        attn_mask = attn_mask.unsqueeze(2).repeat(1,1,self.x_shape[1])\n",
    "        attn_mask = attn_mask.repeat(8,1,1)\n",
    "        \n",
    "        x = self.l1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.l2(x)\n",
    "        \n",
    "        x = x + self.pe1\n",
    "        \n",
    "        # Encoder\n",
    "        _x = x\n",
    "        x = self.attn1(x, x, x, attn_mask=attn_mask)[0]\n",
    "        x = self.ln1(x + _x)\n",
    "        \n",
    "        _x = x\n",
    "        x = self.l3(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.ln2(x + _x)\n",
    "        \n",
    "        _x = x\n",
    "        x = self.attn2(x, x, x, attn_mask=attn_mask)[0]\n",
    "        x = self.ln3(x + _x)\n",
    "        \n",
    "        _x = x\n",
    "        x = self.l5(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.l6(x)\n",
    "        enc_out = self.ln4(x + _x)\n",
    "        \n",
    "        # Decoder\n",
    "        y = y.to(torch.int64)\n",
    "        y = F.pad(y, (0,self.x_shape[1] - y.shape[1],0,0), \"constant\", 61)\n",
    "        y = self.emb1(y)\n",
    "        \n",
    "        x = y + self.pe2\n",
    "        \n",
    "        causal_mask = torch.tril((attn_mask.abs() + 1).bool().float())\n",
    "        \n",
    "        _x = x\n",
    "        x = self.attn3(x, x, x, attn_mask=causal_mask, is_causal=True)[0]\n",
    "        x = self.ln5(x + _x)\n",
    "        \n",
    "        ###\n",
    "        \n",
    "        _x = x\n",
    "        x = self.attn4(x, enc_out, enc_out, attn_mask=causal_mask, is_causal=True)[0]\n",
    "        x = self.ln6(x + _x)\n",
    "        \n",
    "        _x = x\n",
    "        x = self.l7(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.l8(x)\n",
    "        x = self.ln7(x + _x)\n",
    "        \n",
    "        _x = x\n",
    "        x = self.attn5(x, enc_out, enc_out, attn_mask=causal_mask, is_causal=True)[0]\n",
    "        x = self.ln8(x + _x)\n",
    "        \n",
    "        _x = x\n",
    "        x = self.l9(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.l10(x)\n",
    "        x = self.ln9(x + _x)\n",
    "        \n",
    "        x = x[:,:self.y_shape[2]-1,:]\n",
    "        # Classifier\n",
    "        x = self.l11(x)\n",
    "        x = F.softmax(x, 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        ...\n",
    "    \n",
    "    def __call__(self, pred, target):\n",
    "        pred = pred.argmax(2)\n",
    "        target = target.argmax(2)\n",
    "        \n",
    "        acc = []\n",
    "        for p, t in zip(pred, target):\n",
    "            t = t[torch.where((t == 61) | (t == 60), 0, 1).bool()]\n",
    "            p = p[:len(t)]\n",
    "            acc.append((p == t).float().mean())\n",
    "        return sum(acc)/len(acc)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCH = 30\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DSProcessed()\n",
    "train_dl = DataLoader(train_ds, BATCH_SIZE, True, num_workers=1)\n",
    "x, y = next(iter(train_dl))\n",
    "loss_fc = nn.CrossEntropyLoss()\n",
    "accuracy_fc = Accuracy()\n",
    "writer = SummaryWriter(\"runs/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 807, 63]), torch.Size([16, 45]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(x.shape, (y.shape[0], 61, y.shape[1])).to(DEVICE)\n",
    "optim = torch.optim.Adam(model.parameters(), LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \"min\", 0.9, 200, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2_373_438'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{sum([param[1].numel() for param in model.named_parameters()]):_}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dl,\n",
    "    model,\n",
    "    loss_fc,\n",
    "    accuracy_fc,\n",
    "    optim,\n",
    "    device,\n",
    "    epoch,\n",
    "    writer,\n",
    "    scheduler,\n",
    "):\n",
    "    size = len(dl)\n",
    "    model.train()\n",
    "    for batch, (x, y) in enumerate(dl):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        target = one_hot(y[:,1:], 62)\n",
    "\n",
    "        pred = model(x, y)\n",
    "        loss = loss_fc(pred, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        loss = loss.item()\n",
    "        accuracy = accuracy_fc(pred, target)\n",
    "        lr = optim.param_groups[0]['lr']\n",
    "        \n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        writer.add_scalar(\"loss\", loss, (epoch + (batch/size))*1e5)\n",
    "        writer.add_scalar(\"accuracy\", accuracy, (epoch + (batch/size))*1e5)\n",
    "        writer.add_scalar(\"learnig_rate\", lr, (epoch + (batch/size))*1e5)\n",
    "        print(f\"    [{batch+1:>5d}/{size:>5d}] | loss: {loss:>7f} | accuracy: {accuracy:.2%} | lr: {lr}{' '*20}\", end=\"\\r\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for e in range(EPOCH):\n",
    "        print(f\"\\n{'-'*10} epoch: {e+1} {'-'*10}\")\n",
    "        train(train_dl, model, loss_fc, accuracy_fc, optim, DEVICE, e, writer, scheduler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4032 spidel roadEPPPPPPPPPPPPPPPPPPPPPPPPPPP\n",
      "_s~ug5x.~d9.5q.udcPPPPPPPPPPPPPPPPPPPPPPPPPP\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter_dl)\n",
    "print(CTP().list_pred_to_string(y[0].int().tolist())[1:])\n",
    "print(CTP().list_pred_to_string(model(x.to(DEVICE), y.to(DEVICE)).to(\"cpu\").detach().argmax(2)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save / Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "with open(\"./models/va1.torch\", \"wb\") as f: torch.save(model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./models/v1.torch\", \"rb\") as f: model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\").eval()\n",
    "x, y = x.to(\"cpu\"), y.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, {\"x\": x, \"y\": y}, \"./models/v1.onnx\", opset_version=14, input_names=[\"x\", \"y\"], output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"./models/v1.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "BackendIsNotSupposedToImplementIt",
     "evalue": "in user code:\n\n    File \"/home/guy/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/onnx_tf/backend_tf_module.py\", line 99, in __call__  *\n        output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,\n    File \"/home/guy/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/onnx_tf/backend.py\", line 347, in _onnx_node_to_tensorflow_op  *\n        return handler.handle(node, tensor_dict=tensor_dict, strict=strict)\n    File \"/home/guy/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/onnx_tf/handlers/handler.py\", line 61, in handle  *\n        raise BackendIsNotSupposedToImplementIt(\"{} version {} is not implemented.\".format(node.op_type, cls.SINCE_VERSION))\n\n    BackendIsNotSupposedToImplementIt: Unsqueeze version 13 is not implemented.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBackendIsNotSupposedToImplementIt\u001b[0m         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prepare(onnx_model)\u001b[39m.\u001b[39;49mexport_graph(\u001b[39m\"\u001b[39;49m\u001b[39m./models/v1.tf\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/onnx_tf/backend_rep.py:143\u001b[0m, in \u001b[0;36mTensorflowRep.export_graph\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Export backend representation to a Tensorflow proto file.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[39mThis function obtains the graph proto corresponding to the ONNX\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39m:returns: none.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_module\u001b[39m.\u001b[39mis_export \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    140\u001b[0m tf\u001b[39m.\u001b[39msaved_model\u001b[39m.\u001b[39msave(\n\u001b[1;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_module,\n\u001b[1;32m    142\u001b[0m     path,\n\u001b[0;32m--> 143\u001b[0m     signatures\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtf_module\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m\u001b[39m.\u001b[39;49mget_concrete_function(\n\u001b[1;32m    144\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignatures))\n\u001b[1;32m    145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_module\u001b[39m.\u001b[39mis_export \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1258\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1257\u001b[0m   \u001b[39m# Implements GenericFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1258\u001b[0m   concrete \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1259\u001b[0m   concrete\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m   \u001b[39mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1238\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1236\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1237\u001b[0m     initializers \u001b[39m=\u001b[39m []\n\u001b[0;32m-> 1238\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwargs, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[1;32m   1239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[1;32m   1242\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m   \u001b[39m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:763\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph \u001b[39m=\u001b[39m lifted_initializer_graph\n\u001b[1;32m    761\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_deleter \u001b[39m=\u001b[39m FunctionDeleter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph)\n\u001b[1;32m    762\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn    \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    764\u001b[0m     \u001b[39m.\u001b[39;49m_get_concrete_function_internal_garbage_collected(\n\u001b[1;32m    765\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[1;32m    768\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:171\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m--> 171\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_concrete_function(args, kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:166\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[1;32m    164\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 166\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:396\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m   args \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39margs\n\u001b[1;32m    394\u001b[0m kwargs \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39mkwargs\n\u001b[0;32m--> 396\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(\n\u001b[1;32m    397\u001b[0m     args, kwargs, func_graph)\n\u001b[1;32m    399\u001b[0m \u001b[39m# TODO(b/263520817): Remove access to private attribute.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_function_captures  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:300\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m   arg_names \u001b[39m=\u001b[39m base_arg_names\n\u001b[1;32m    299\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 300\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m    301\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m    302\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m    303\u001b[0m         args,\n\u001b[1;32m    304\u001b[0m         kwargs,\n\u001b[1;32m    305\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    306\u001b[0m         func_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[1;32m    307\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m    308\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m    309\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m    310\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value,\n\u001b[1;32m    311\u001b[0m         create_placeholders\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    312\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m    313\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m    314\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    319\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1214\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1212\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1214\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1216\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:667\u001b[0m, in \u001b[0;36mFunction._compiler_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    664\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    665\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    666\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 667\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39;49m__wrapped__(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    668\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:484\u001b[0m, in \u001b[0;36mclass_method_to_instance_method.<locals>.bound_method_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapped_fn(weak_instance(), \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    481\u001b[0m \u001b[39m# If __wrapped__ was replaced, then it is always an unbound function.\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[39m# However, the replacer is still responsible for attaching self properly.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[39m# TODO(mdan): Is it possible to do it here instead?\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1200\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1200\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   1201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1202\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1189\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m   \u001b[39mreturn\u001b[39;00m autograph\u001b[39m.\u001b[39;49mconverted_call(\n\u001b[1;32m   1190\u001b[0m       original_func,\n\u001b[1;32m   1191\u001b[0m       args,\n\u001b[1;32m   1192\u001b[0m       kwargs,\n\u001b[1;32m   1193\u001b[0m       options\u001b[39m=\u001b[39;49mautograph\u001b[39m.\u001b[39;49mConversionOptions(\n\u001b[1;32m   1194\u001b[0m           recursive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1195\u001b[0m           optional_features\u001b[39m=\u001b[39;49mautograph_options,\n\u001b[1;32m   1196\u001b[0m           user_requested\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1197\u001b[0m       ))\n\u001b[1;32m   1198\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file18jds678.py:30\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m node \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mnode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m onnx_node \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39monnx_node\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m ag__\u001b[39m.\u001b[39;49mfor_stmt(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mgraph_def\u001b[39m.\u001b[39;49mnode, \u001b[39mNone\u001b[39;49;00m, loop_body, get_state, set_state, (), {\u001b[39m'\u001b[39;49m\u001b[39miterate_names\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mnode\u001b[39;49m\u001b[39m'\u001b[39;49m})\n\u001b[1;32m     31\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mdict\u001b[39m), (), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_4\u001b[39m():\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:451\u001b[0m, in \u001b[0;36mfor_stmt\u001b[0;34m(iter_, extra_test, body, get_state, set_state, symbol_names, opts)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(iter_, distribute\u001b[39m.\u001b[39mIterable):\n\u001b[1;32m    448\u001b[0m   \u001b[39m# TODO(b/162250181): Use _tf_iterator_for_stmt(iter(iter_)...\u001b[39;00m\n\u001b[1;32m    449\u001b[0m   for_fn \u001b[39m=\u001b[39m _tf_distributed_iterable_for_stmt\n\u001b[0;32m--> 451\u001b[0m for_fn(iter_, extra_test, body, get_state, set_state, symbol_names, opts)\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:502\u001b[0m, in \u001b[0;36m_py_for_stmt\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m   \u001b[39mfor\u001b[39;00m target \u001b[39min\u001b[39;00m iter_:\n\u001b[0;32m--> 502\u001b[0m     body(target)\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:468\u001b[0m, in \u001b[0;36m_py_for_stmt.<locals>.protected_body\u001b[0;34m(protected_iter)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprotected_body\u001b[39m(protected_iter):\n\u001b[0;32m--> 468\u001b[0m   original_body(protected_iter)\n\u001b[1;32m    469\u001b[0m   after_iteration()\n\u001b[1;32m    470\u001b[0m   before_iteration()\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file18jds678.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     21\u001b[0m node \u001b[39m=\u001b[39m itr\n\u001b[1;32m     22\u001b[0m onnx_node \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(OnnxNode), (ag__\u001b[39m.\u001b[39mld(node),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 23\u001b[0m output_ops \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mbackend\u001b[39m.\u001b[39;49m_onnx_node_to_tensorflow_op, (ag__\u001b[39m.\u001b[39;49mld(onnx_node), ag__\u001b[39m.\u001b[39;49mld(tensor_dict), ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mhandlers), \u001b[39mdict\u001b[39;49m(opset\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mopset, strict\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mstrict), fscope)\n\u001b[1;32m     24\u001b[0m curr_node_output_map \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mdict\u001b[39m), (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mzip\u001b[39m), (ag__\u001b[39m.\u001b[39mld(onnx_node)\u001b[39m.\u001b[39moutputs, ag__\u001b[39m.\u001b[39mld(output_ops)), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     25\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tensor_dict)\u001b[39m.\u001b[39mupdate, (ag__\u001b[39m.\u001b[39mld(curr_node_output_map),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileica28nb4.py:62\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___onnx_node_to_tensorflow_op\u001b[0;34m(cls, node, tensor_dict, handlers, opset, strict)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     61\u001b[0m handler \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mhandler\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mld(handlers), if_body_1, else_body_1, get_state_1, set_state_1, (\u001b[39m'\u001b[39;49m\u001b[39mdo_return\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mretval_\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m2\u001b[39;49m)\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_2\u001b[39m():\n\u001b[1;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m ()\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1266\u001b[0m, in \u001b[0;36mif_stmt\u001b[0;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[1;32m   1264\u001b[0m   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)\n\u001b[1;32m   1265\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1266\u001b[0m   _py_if_stmt(cond, body, orelse)\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1319\u001b[0m, in \u001b[0;36m_py_if_stmt\u001b[0;34m(cond, body, orelse)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_py_if_stmt\u001b[39m(cond, body, orelse):\n\u001b[1;32m   1318\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Overload of if_stmt that executes a Python if statement.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1319\u001b[0m   \u001b[39mreturn\u001b[39;00m body() \u001b[39mif\u001b[39;00m cond \u001b[39melse\u001b[39;00m orelse()\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileica28nb4.py:56\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___onnx_node_to_tensorflow_op.<locals>.if_body_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mnonlocal\u001b[39;00m do_return, retval_\n\u001b[1;32m     55\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mld(handler), if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39;49m\u001b[39mdo_return\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mretval_\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1266\u001b[0m, in \u001b[0;36mif_stmt\u001b[0;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[1;32m   1264\u001b[0m   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)\n\u001b[1;32m   1265\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1266\u001b[0m   _py_if_stmt(cond, body, orelse)\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1319\u001b[0m, in \u001b[0;36m_py_if_stmt\u001b[0;34m(cond, body, orelse)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_py_if_stmt\u001b[39m(cond, body, orelse):\n\u001b[1;32m   1318\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Overload of if_stmt that executes a Python if statement.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1319\u001b[0m   \u001b[39mreturn\u001b[39;00m body() \u001b[39mif\u001b[39;00m cond \u001b[39melse\u001b[39;00m orelse()\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileica28nb4.py:48\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___onnx_node_to_tensorflow_op.<locals>.if_body_1.<locals>.if_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(handler)\u001b[39m.\u001b[39;49mhandle, (ag__\u001b[39m.\u001b[39;49mld(node),), \u001b[39mdict\u001b[39;49m(tensor_dict\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(tensor_dict), strict\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(strict)), fscope)\n\u001b[1;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file459dpsg3.py:41\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__handle\u001b[0;34m(cls, node, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[39mnonlocal\u001b[39;00m do_return, retval_\n\u001b[1;32m     40\u001b[0m     \u001b[39mraise\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(BackendIsNotSupposedToImplementIt), (ag__\u001b[39m.\u001b[39mconverted_call(\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m version \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is not implemented.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat, (ag__\u001b[39m.\u001b[39mld(node)\u001b[39m.\u001b[39mop_type, ag__\u001b[39m.\u001b[39mld(\u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39mSINCE_VERSION), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 41\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mld(ver_handle), if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39;49m\u001b[39mdo_return\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mretval_\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m2\u001b[39;49m)\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m fscope\u001b[39m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1266\u001b[0m, in \u001b[0;36mif_stmt\u001b[0;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[1;32m   1264\u001b[0m   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)\n\u001b[1;32m   1265\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1266\u001b[0m   _py_if_stmt(cond, body, orelse)\n",
      "File \u001b[0;32m~/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1319\u001b[0m, in \u001b[0;36m_py_if_stmt\u001b[0;34m(cond, body, orelse)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_py_if_stmt\u001b[39m(cond, body, orelse):\n\u001b[1;32m   1318\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Overload of if_stmt that executes a Python if statement.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1319\u001b[0m   \u001b[39mreturn\u001b[39;00m body() \u001b[39mif\u001b[39;00m cond \u001b[39melse\u001b[39;00m orelse()\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file459dpsg3.py:40\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__handle.<locals>.else_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39melse_body\u001b[39m():\n\u001b[1;32m     39\u001b[0m     \u001b[39mnonlocal\u001b[39;00m do_return, retval_\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mraise\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(BackendIsNotSupposedToImplementIt), (ag__\u001b[39m.\u001b[39mconverted_call(\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m version \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is not implemented.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat, (ag__\u001b[39m.\u001b[39mld(node)\u001b[39m.\u001b[39mop_type, ag__\u001b[39m.\u001b[39mld(\u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39mSINCE_VERSION), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mBackendIsNotSupposedToImplementIt\u001b[0m: in user code:\n\n    File \"/home/guy/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/onnx_tf/backend_tf_module.py\", line 99, in __call__  *\n        output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,\n    File \"/home/guy/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/onnx_tf/backend.py\", line 347, in _onnx_node_to_tensorflow_op  *\n        return handler.handle(node, tensor_dict=tensor_dict, strict=strict)\n    File \"/home/guy/Desktop/Google-American-Sign-Language-Fingerspelling-Recognition/venv/lib/python3.10/site-packages/onnx_tf/handlers/handler.py\", line 61, in handle  *\n        raise BackendIsNotSupposedToImplementIt(\"{} version {} is not implemented.\".format(node.op_type, cls.SINCE_VERSION))\n\n    BackendIsNotSupposedToImplementIt: Unsqueeze version 13 is not implemented.\n"
     ]
    }
   ],
   "source": [
    "prepare(onnx_model).export_graph(\"./models/v1.tf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
